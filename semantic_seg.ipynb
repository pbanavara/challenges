{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_order_images(img):\n",
    "    classes = [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11]\n",
    "    new_img = np.zeros((img.shape[0], img.shape[1], len(classes)))\n",
    "    for i, x in enumerate(classes):\n",
    "        x, y = np.where(img[:, :, 0] == x/255)\n",
    "        new_img[x, y, i] = img[x, y, 0]\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(label_image):\n",
    "    labels_new = np.copy(label_image)\n",
    "    # Identify lane marking pixels (label is 6)\n",
    "    # Set lane marking pixels to road (label is 7)\n",
    "    labels_new[np.where(label_image[:, :, 0] == 6/255)] = 7/255\n",
    "    \n",
    "    # Identify all vehicle pixels\n",
    "    vehicle_pixels = (label_image[:,:,0] == 10/255).nonzero()\n",
    "    # Isolate vehicle pixels associated with the hood (y-position > 496)\n",
    "    hood_indices = (vehicle_pixels[0] >= 496).nonzero()[0]\n",
    "    hood_pixels = (vehicle_pixels[0][hood_indices], \\\n",
    "                   vehicle_pixels[1][hood_indices])\n",
    "    # Set hood pixel labels to 0\n",
    "    labels_new[hood_pixels] = 0\n",
    "    new_img = re_order_images(labels_new)\n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.path.curdir\n",
    "dir_name = os.path.join(cwd, '../data/Train/CameraSeg/')\n",
    "label_arr = []\n",
    "for filename in os.listdir(dir_name):\n",
    "    filename = os.path.join(dir_name, filename)\n",
    "    label = plt.imread(filename)\n",
    "    label_arr.append(preprocess_labels(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b6d03ff77f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = np.stack(label_arr)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1821c24438>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1821c6eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "plt.imshow(random.choice(labels)[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_name = os.path.join(cwd, 'Train/CameraRGB/')\n",
    "image_arr = []\n",
    "for filename in os.listdir(image_dir_name):\n",
    "    filename = os.path.join(image_dir_name, filename)\n",
    "    image = plt.imread(filename)\n",
    "    image_arr.append(image)\n",
    "images = np.array(image_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 600, 800, 11) (1000, 600, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape, images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(size):\n",
    "    for i in range(0, images.shape[0], size):\n",
    "        batch_images = images[i:i+size]\n",
    "        batch_labels = labels[i:i+size]\n",
    "        batch_images, batch_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'vgg/variables/variables'\n"
     ]
    }
   ],
   "source": [
    "def load_vgg_pretrained(sess, vgg_path):\n",
    "    vgg_tag = 'vgg16'\n",
    "    vgg_input_tensor_name = 'image_input:0'\n",
    "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "    graph = tf.get_default_graph()\n",
    "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "    w1 = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "    layer3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "    layer4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "    layer7 = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "    return w1, keep_prob, layer3, layer4, layer7\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    load_vgg_pretrained(sess, 'vgg/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    \"\"\"\n",
    "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "    :param vgg_layer7_out: TF Tensor for VGG Layer 3 output\n",
    "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
    "    :param vgg_layer3_out: TF Tensor for VGG Layer 7 output\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: The Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "    # Layer 7 upsampling\n",
    "    layer7_out = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, \n",
    "                                   padding= 'same', \n",
    "                                   kernel_initializer= tf.random_normal_initializer(stddev=0.01),\n",
    "                                   kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    # upsample\n",
    "    layer4_in = tf.layers.conv2d_transpose(layer7_out, num_classes, 4, \n",
    "                                             strides= (2, 2), \n",
    "                                             padding= 'same', \n",
    "                                             kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                             kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "   \n",
    "    layer4_in2 = tf.layers.conv2d(vgg_layer4_out, num_classes, 1, \n",
    "                                   padding= 'same', \n",
    "                                   kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                   kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    # skip connection (element-wise addition)\n",
    "    layer4_out = tf.add(layer4_in, layer4_in2)\n",
    "    layer3_in = tf.layers.conv2d_transpose(layer4_out, num_classes, 4, \n",
    "                                             strides= (2, 2), \n",
    "                                             padding= 'same', \n",
    "                                             kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                             kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    \n",
    "    layer3_in2 = tf.layers.conv2d(vgg_layer3_out, num_classes, 1, \n",
    "                                   padding= 'same', \n",
    "                                   kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                   kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    layer3_out = tf.add(layer3_in, layer3_in2)\n",
    "    \n",
    "    last_layer = tf.layers.conv2d_transpose(layer3_out, num_classes, 16,  \n",
    "                                               strides= (8, 8), \n",
    "                                               padding= 'same', \n",
    "                                               kernel_initializer= tf.random_normal_initializer(stddev=0.01), \n",
    "                                               kernel_regularizer= tf.contrib.layers.l2_regularizer(1e-3))\n",
    "    return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Build the TensorFLow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label image\n",
    "    :param learning_rate: TF Placeholder for the learning rate\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    labels = tf.reshape(correct_label, (-1, num_classes))\n",
    "    print(nn_last_layer.shape, labels.shape)\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits, \n",
    "                                                                                labels= labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy_loss)\n",
    "    return logits, train_op, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, train_op, cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate):\n",
    "    \"\"\"\n",
    "    Train neural network and print out the loss during training.\n",
    "    :param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param correct_label: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    for epoch in range(epochs):\n",
    "        s_time = time.time()\n",
    "        for i in range(0, images.shape[0], batch_size):\n",
    "            image = images[i:i+batch_size]\n",
    "            targets = labels[i:i+batch_size]\n",
    "            _, loss = sess.run([train_op, cross_entropy_loss], \n",
    "                feed_dict = {input_image: image, correct_label: targets, keep_prob: 0.5 ,\n",
    "                             learning_rate: 1e-4 })\n",
    "        print(\"Epoch: {}\".format(epoch + 1), \"/ {}\".format(epochs), \" Loss: {:.3f}\".format(loss), \" Time: \",\n",
    "              str(timedelta(seconds=(time.time() - s_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image):\n",
    "    # Make folder for current run\n",
    "    output_dir = os.path.join(runs_dir, str(time.time()))\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    # Run NN on test images and save them to HD\n",
    "    print('Training Finished. Saving test images to: {}'.format(output_dir))\n",
    "    image_outputs = gen_test_output(\n",
    "        sess, logits, keep_prob, input_image, os.path.join(data_dir, 'data_road/testing'), image_shape)\n",
    "    for name, image in image_outputs:\n",
    "        scipy.misc.imsave(os.path.join(output_dir, name), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    num_classes = 11\n",
    "    image_shape = (600, 800)\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    #tests.test_for_kitti_dataset(data_dir)\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    #helper.maybe_download_pretrained_vgg(data_dir)\n",
    "    with tf.Session() as sess:\n",
    "        vgg_path = os.path.join(cwd, 'vgg')\n",
    "        epochs = 100\n",
    "        batch_size = 5\n",
    "        correct_label = tf.placeholder(tf.int32, [None, None, None, num_classes], name='correct_label')\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg_pretrained(sess, vgg_path)\n",
    "        #print(input_image)\n",
    "        nn_last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
    "        logits, train_op, cross_entropy_loss = optimize(nn_last_layer, correct_label, \n",
    "                                                        learning_rate, num_classes)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_nn(sess, epochs, batch_size, train_op, cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate)\n",
    "\n",
    "        # TODO: Save inference data using helper.save_inference_samples\n",
    "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./vgg/variables/variables'\n",
      "(?, ?, ?, 11) (?, 11)\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
